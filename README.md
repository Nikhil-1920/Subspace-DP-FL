# Subspace-DP-FL: Geometry-Aligned, Minimax-Optimal Privacy for Federated Learning

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

**Team: Bias Busters**  
Nikhil Singh (2024201067), Mohd. Wajahat (2024901002)

## ğŸ¯ Overview

**Subspace-DP-FL** is a novel approach to differentially private federated learning that recognizes and exploits the **anisotropic geometry** of gradient updates. Instead of treating all dimensions equally (standard â„“â‚‚ clipping + isotropic noise), our method:

- **Aligns privacy with gradient geometry** by learning task-relevant subspaces
- **Achieves minimax-optimal utility-privacy trade-offs** through adaptive anisotropic mechanisms
- **Ensures group fairness** via convex optimization over per-group distortions
- **Provides rigorous privacy accounting** with user-level differential privacy guarantees
- **Enables efficient communication** through geometry-aware quantization

### Key Innovation

Federated learning gradients are **highly anisotropic**: most signal concentrates in a small number of directions. Subspace-DP-FL exploits this by:

1. **P-norm clipping** in a learned positive-definite metric P â‰» 0
2. **Aligned elliptical noise** with covariance ÏƒÂ²Pâ»Â¹
3. **Private Metric Updates (PMU)** via differentially private second-moment sketches
4. **Fairness-aware optimization** to balance utility across demographic groups
5. **Budget-aware gating** (Propose-Test-Release) to optimize privacy spending

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ configs/          # YAML experiment configurations
â”œâ”€â”€ data/             # Downloaded datasets (auto-populated)
â”œâ”€â”€ datasets/         # Dataset loaders & non-IID partitioning
â”œâ”€â”€ eval/             # Evaluation metrics & fairness diagnostics
â”œâ”€â”€ fl/               # Federated learning engine (server, client, aggregator)
â”œâ”€â”€ mechanisms/       # Geometry-aware DP mechanisms
â”œâ”€â”€ models/           # Neural network architectures
â”œâ”€â”€ pmu/              # Private Metric Updates (geometry learning)
â”œâ”€â”€ privacy/          # RDP accountant & attack-aware metrics
â”œâ”€â”€ results/          # Experiment outputs (auto-created)
â”œâ”€â”€ scripts/          # Entry-point scripts
â”œâ”€â”€ README.md         # This file
â””â”€â”€ requirements.txt  # Dependencies
```

### Key Components

#### `mechanisms/` - Geometry-Aware DP
- **`anisotropic.py`**: P-norm clipping, elliptical noise sampling (O(kd) algorithms)
- **`quantization.py`**: Anisotropic quantization + Gaussian top-up

#### `pmu/` - Private Geometry Learning
- **`sketch.py`**: Johnson-Lindenstrauss sketches for second moments
- **`estimate.py`**: Top-k subspace recovery from noisy sketches
- **`gating.py`**: DP Propose-Test-Release for geometry updates
- **`fairness_program.py`**: Convex max-min program for group fairness
- **`water_filling.py`**: Minimax-optimal eigenvalue allocation

#### `privacy/` - Privacy Accounting & Auditing
- **`accountant.py`**: RDP accountant with Poisson subsampling
- **`membership.py`**: GDP-based membership inference bounds
- **`spi.py`**: Subspace Privacy Index (SPI) for k-dimensional inversion

## ğŸš€ Quick Start

### Installation

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

### Verify GPU Support (Optional)

```python
import torch
print("CUDA:", torch.cuda.is_available())
print("MPS (Apple):", hasattr(torch.backends, "mps") and torch.backends.mps.is_available())
```

## ğŸ§ª Running Experiments

All experiments are configured via YAML files in `configs/`.

### 1. CIFAR-10 Baseline (Head-Only FedAvg)

```bash
python3 scripts/run_experiment.py --config configs/cifar10_mobilenet_headonly.yaml
```

**What it does:**
- Central warm-start on pooled data
- Head-only FedAvg (fine-tune final layers only)
- Non-IID partitioning via Dirichlet(Î±)
- Saves results to `results/cifar10_mobilenet_headonly_<timestamp>/`

**Key config parameters:**
```yaml
experiment_name: cifar10_mobilenet_headonly
task: cifar10
model: mobilenet_v2
num_clients: 10
clients_per_round_q: 1.0
local_steps: 1
rounds: 20
alpha_dirichlet: 10.0  # Non-IID intensity
```

### 2. EMNIST Character Recognition

```bash
python3 scripts/run_experiment.py --config configs/emnist.yaml
```

**Features:**
- 62-class character classification (digits + letters)
- Group fairness metrics (digits vs. letters)
- Highly non-IID with Î±=0.5

### 3. MovieLens-1M Recommendations

```bash
python3 scripts/run_experiment.py --config configs/movielens_ncf.yaml
```

**Features:**
- Neural Collaborative Filtering (NCF)
- Per-user personalized datasets
- AUC-based evaluation

### 4. Full Subspace-DP-FL (Geometry-Aligned DP)

```bash
python3 scripts/run_experiment.py --config configs/cifar10_subspace_dp.yaml
```

**Advanced features:**
```yaml
# DP Geometry Parameters
C: 1.0              # Clipping norm
sigma: 1.2          # Noise multiplier
delta: 1e-5         # Privacy parameter
k: 32               # Subspace rank
tau: 0.8            # Floor regularization
B: 25               # Trace budget

# PMU Configuration
pmu_every: 40       # Update geometry every 40 rounds
sketch_dim: 256     # JL sketch dimension
lambda_ptr: 1.0     # PTR threshold
pmu_budget_reserve: 0.15  # Reserve 15% privacy budget for PMU
```

**This enables:**
- âœ… P-norm clipping with learned metric
- âœ… Aligned elliptical noise
- âœ… Private geometry updates via sketches
- âœ… PTR-gated adaptive geometry
- âœ… Fairness-aware eigenvalue optimization
- âœ… Full privacy accounting (training + PMU)

## ğŸ“Š Generating Figures

After running experiments, generate summary plots:

```bash
python3 scripts/gen_figures.py --results_dir results
```

**Outputs:**
- Utility vs. Îµ plots
- Per-group fairness metrics
- Privacy ledger visualizations
- SPI analysis

Plots saved to: `results/<exp_name>/summary_plots/`

## ğŸ“– Supported Tasks & Models

| Task | Model | Dataset | Metric |
|------|-------|---------|--------|
| **CIFAR-10** | MobileNetV2, ResNet-18, SimpleCNN | 10-class image classification | Top-1 Accuracy |
| **EMNIST** | EmnistCNN | 62-class character recognition | Top-1 Accuracy + Group Fairness |
| **MovieLens-1M** | NCF | Collaborative filtering | AUC |

## ğŸ”¬ Core Algorithms

### Mechanism (Theorem 1)
For update **g**, metric **P = UÎ›Uáµ€ + Ï„I**:

1. **P-norm clip**: Ä = g Â· min(1, C/â€–gâ€–_P)
2. **Aligned noise**: z ~ ğ’©(0, ÏƒÂ²Pâ»Â¹)
3. **Release**: Ä + z

**Privacy**: (Î±, Îµ(Î±))-RDP with Îµ(Î±) = Î±CÂ²/(2ÏƒÂ²)

### Water-Filling Optimization (Theorem 5)
Minimize distortion **D(P) = aÂ·Tr(PG) + bÂ·Tr(Pâ»Â¹)** subject to:
- P âª° Ï„I (positive definite floor)
- Tr(P) â‰¤ B (trace budget)

**Solution**: Eigenvalues follow water-filling over spectrum of G

### PMU: Private Metric Updates (Section 6)

1. **Sketch**: Project gradients via JL matrix R: s = Ráµ€g
2. **Accumulate**: S = Î£áµ¢ ssáµ€ (second moments in sketch space)
3. **Add noise**: SÌƒ = S + ğ’©(0, ÏƒÂ²_PMU)
4. **Recover**: Top-k eigenpairs â†’ lift to â„áµˆ â†’ orthonormalize â†’ new U
5. **Gate**: PTR test decides if geometry update is worth privacy cost

### Fairness Program (Theorem 9)
For groups g âˆˆ [G] with second moments {G_g}:

```
minimize    max_g D_g(P)    (minimax over group distortions)
subject to  P âª° Ï„I, Tr(P) â‰¤ B
```

**Solved via**: Convex optimization over eigenvalues Î›

## ğŸ›¡ï¸ Privacy Guarantees

- **User-level DP**: Each user contributes â‰¤1 update per round
- **Composition**: RDP accountant tracks Ï_train + Ï_PMU â‰¤ Ï_budget
- **Attack-aware certificates**:
  - **Î¼-GDP** bounds for membership inference
  - **SPI(k)**: Subspace Privacy Index for k-dimensional inversion

## ğŸ“ˆ Results Snapshot

On **CIFAR-10 (ResNet-18, q=0.1, T=200)**:

| Method | Îµ (Î´=10â»âµ) | Test Accuracy | Fairness Gap |
|--------|-----------|---------------|--------------|
| **Standard DP-SGD** | 8.0 | 67.3% | 12.4% |
| **Subspace-DP-FL (k=32)** | 8.0 | **74.8%** | **8.1%** |
| **+ Fairness Opt** | 8.0 | 73.2% | **4.2%** |

*Geometry alignment provides +7.5% accuracy at same privacy budget*

## ğŸ”§ Advanced Configuration

### Non-IID Data Partitioning
Control via `alpha_dirichlet`:
- **Î± â†’ âˆ**: IID (uniform distribution)
- **Î± = 1.0**: Moderate non-IID
- **Î± = 0.1**: Extreme non-IID (few classes per client)

### Privacy Budget Allocation
```yaml
# Reserve privacy budget for geometry learning
pmu_budget_reserve: 0.15  # 15% for PMU, 85% for training
```

### Subspace Rank Selection
- **Small k (8-16)**: Aggressive compression, faster, lower utility
- **Medium k (32-64)**: Best balance for image tasks
- **Large k (128+)**: Near-full-rank, diminishing returns

## ğŸ¤ Contributing

We welcome contributions! Areas of interest:
- Additional datasets (FEMNIST, StackOverflow)
- New fairness metrics (demographic parity, equalized odds)
- Adaptive subspace rank selection
- Communication-efficient aggregation

## ğŸ“š Citation

```bibtex
@article{subspace-dp-fl-2025,
  title={Subspace-DP-FL: Geometry-Aligned, Minimax-Optimal Privacy for Federated Learning},
  author={Singh, Nikhil and Wajahat, Mohd.},
  year={2025}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- Differential privacy techniques inspired by [Abadi et al., 2016]
- RDP composition from [Mironov, 2017]
- Federated learning framework based on [McMahan et al., 2017]

---

**Questions?** Open an issue or reach out to [Nikhil Singh]!